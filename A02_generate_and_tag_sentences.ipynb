{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A02_generate_and_tag_sentences.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":".statbot","language":"python","name":".statbot"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xM3TUnWwZutX"},"source":["# 02 Generate and tag sentences"]},{"cell_type":"markdown","metadata":{"id":"fgq0-qPzyMzS"},"source":["#### Connection to Google Drive - Only execute if you are running this on GD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_16I4-pZrqo","executionInfo":{"status":"ok","timestamp":1616430845208,"user_tz":-60,"elapsed":20235,"user":{"displayName":"Manuela Paganini","photoUrl":"","userId":"16245217794239939786"}},"outputId":"f1f90dfd-dbba-4f69-b4be-6f021d02e70a"},"source":["# set up connection to your google drive\n","# please click on the link generated and enter the authorisation code\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFoJXAS5afk8","executionInfo":{"status":"ok","timestamp":1616430852037,"user_tz":-60,"elapsed":588,"user":{"displayName":"Manuela Paganini","photoUrl":"","userId":"16245217794239939786"}},"outputId":"bbf2323f-faed-47b5-f933-e3dbd9fe68da"},"source":["cd /content/drive/MyDrive/Colab/statbot"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab/statbot\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PokqyErnZrY4"},"source":["! pip install wget"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WNDOagyTW6bz"},"source":["\n","## Script A02\n","\n","This script generates the training dataset, containing questions and the   corresponding tags (such as granularity and datasets) in the Spacy-format so it can be used for the training.\n","\n","It loads a csv-file with pre-written questions that contain brackets to insert elements such as {dataset} or {year} or {granularity} etc.  `(see input/question_generator.csv)` Currently, we only have seven questions, please feel free to add more. For every dataset we have, several questions are being generated randomly.\n","\n","A lot could be improved in this script. At the moment, it only takes data of the so called \"Gemeindeportraet\" of the Statistics Office of the Canton of Zurich. This is because those data variables are in the same format and use the same variable names. We want of course to get away from this structure and use all kind of statistical OGD.\n","\n","## Thoughts for improvement\n","\n","- Include other useful **custom tags**\n","- **Insert important elements** correctly, such as year data, locality level, etc.\n","- Improve **generating the questions**: i.e. by insterting many more ways of saying 'take the three first elements', 'take all elements larger than X', etc. etc.\n","- Optional: Try and **tag every single** word in the datasets as dataset element to improve recognition (and then later use this to attribute them to the right dataset) <br><br>\n","\n","- Outlook for later: Try and **combine this script with the generation of training data** so it can be used for an approach with seq2SQL or similar\n"]},{"cell_type":"code","metadata":{"id":"ES7xxOhwW6cA"},"source":["from __future__ import print_function, unicode_literals\n","import pandas as pd\n","import numpy as np\n","from random import sample\n","import io, csv\n","import re\n","import random\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ui2KdJ0lW6cC"},"source":["def uniform_cleaning(str_in):\n","    str_in=str_in.replace(\"-\",\"\")\n","    str_in=str_in.replace(\"(\",\"\")\n","    str_in=str_in.replace(\")\",\"\")\n","    str_in=str_in.replace(\"ü\",\"ue\")\n","    str_in=str_in.replace(\"ä\",\"ae\")\n","    str_in=str_in.replace(\"ö\",\"oe\")\n","    return(str_in)\n","    \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tc4A3oDYW6cD"},"source":["random.seed(1)\n","\n","df=pd.read_csv(\"data/datasets_overview.csv\")\n","\n","#one more prepatation\n","df['question_type'] = df['dataset_title'].str.extract(r\"\\[(.*?)\\]\", expand=False)\n","df['question_type'] = np.where(df['question_type']== '%', \"percent\", \"cardinal\")\n","df['dataset_title'] = df['dataset_title'].str.replace(r\"\\[(.*?)\\]\", \"\")\n","#structurize relevant informations such as variables\n","\n","#load content of datasets\n","\n","\n","# load template sentences \n","sentences=pd.read_csv(\"input/question_generator.csv\")\n","out_sentences=[]\n","\n","for i in df['index'].unique():\n","    # for each dataset of the gemeindeportraet \n","    filename=\"data/\"+str(i)+\".csv\" \n","    with io.open(filename, 'r', encoding=\"latin-1\") as csvfile:\n","        dialect = csv.Sniffer().sniff(csvfile.readline(), [',',';'])\n","        csvfile.seek(0)\n","        mydelimiter=dialect.delimiter\n","    try:\n","      # open dataset \n","        this_data=pd.read_csv(filename,delimiter=dialect.delimiter)\n","        vars=df['var'].loc[df['index'] == i]\n","        dataset_title=df['dataset_title'].loc[df['index']==i].any()\n","        question_type=df['question_type'].loc[df['index']==i].any()\n","        # at the moment: only take columns containing the main value of the gemeindeportraet dataset----------\n","        if vars.str.contains(\"INDIKATOR_VALUE\").sum():\n","            #title of the column \n","            main=uniform_cleaning(dataset_title.strip())\n","            # values of the column  \n","            filter_vars=vars\n","            #print(filter_vars)\n","        else:\n","          #skip this column (temporary so all variables look alike) \n","            continue \n","            print(\"ATTRIBUTING MAIN VARIABLE TO: \",dataset_title)\n","            length_title_string=len(dataset_title.split())\n","            highest_similarity=0\n","            which=None\n","            for var in vars:\n","                temp_string=dataset_title+\" \"+var\n","                #print(temp_string)\n","                doc = nlp(temp_string)\n","                assert len(doc.vector) == len(doc[0].vector)\n","                calc_similarity=doc[:length_title_string].similarity(doc[length_title_string:])\n","                if  calc_similarity> highest_similarity:\n","                    highest_similarity=calc_similarity\n","                    which=var\n","            print(\"Highest similarity:\",var,highest_similarity)\n","            main=uniform_cleaning(var)\n","            vars=vars.tolist()\n","            print(vars)\n","            print(main)\n","            filter_vars=vars.remove(main)\n","\n","        #the following temporary because it is standardized\n","        filter_vars=list(filter_vars)\n","        filter_vars.append(\"\")\n","        try:\n","            filter_vars.remove(\"INDIKATOR_JAHR\")\n","            filter_vars.remove(\"GEBIET_NAME\")\n","            filter_vars.remove(\"BFS_NR\")\n","            filter_vars.remove(\"INDIKATOR_VALUE\")\n","        except:\n","            print(\"variable removing empty\")\n","\n","        # generate sentences \n","        for sentence in sentences['question'].loc[sentences['main_type'] == question_type]:\n","            out_entities = []\n","            # replace placeholder with column title extracted above  \n","            sentence=sentence.replace(\"{main}\",main)\n","            sentence=sentence.replace(\"{localitylevel}\",\"\")#at the moment empty\n","            #TODO either one locality, one level, or several localities\n","            random_value=sample([\"one locality\",\"one level\",\"several localities\"],1)[0]\n","            if random_value==\"one locality\":\n","                locality_insert=\"in \"+sample(list(this_data['GEBIET_NAME']),1)[0]\n","            if random_value==\"one level\":\n","                locality_insert=sample([\"für den gesamten Kanton\",\"im Kanton Zürich\",\"auf Bezirksebene\",\n","                \"für alle Bezirke\",\"pro Bezirk\",\"auf Gemeindeebene\",\"für alle Gemeinden\",\"pro Gemeinde\"],1)[0]\n","            if random_value==\"several localities\":\n","                locality_insert=\"\"\n","                local_loop=sample([1,2,3],1)[0]\n","                for local in range(0,local_loop):\n","                    if local!=0 and local!=(local_loop-1):\n","                        locality_insert+=\", \"\n","                    if local!=0 and local==(local_loop-1):\n","                        locality_insert+=\" und \"\n","                    locality_insert+=sample(list(this_data['GEBIET_NAME']),1)[0]\n","            sentence=sentence.replace(\"{locality}\",locality_insert)\n","            sentence=sentence.replace(\"{yeartime}\",\"\")#TODO no,aktuellste,neuste, value from list, from to year\n","            sentence=sentence.replace(\"{filter}\",sample(list(filter_vars),1)[0])\n","\n","            for mat in re.findall(r'.*?\\[(.*)].*', sentence):\n","                which_part=sample([1,2],1)\n","\n","                if which_part==1:\n","                    sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[0])\n","                    #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[0],sentence)\n","                else:\n","                    sentence=sentence.replace(\"[\"+mat+\"]\",mat.partition(\"|\")[2])\n","                    #sentence=re.sub(\"[\"+mat+\"]\",mat.partition(\"|\")[2],sentence)\n","\n","\n","            #now the symbol - has to be deleted as it gives issues\n","            sentence=uniform_cleaning(sentence)\n","\n","\n","            #### TAGGING\n","\n","            #1) GRAN\n","            # check if sentence contains granularity \n","            match_span = re.search(r'\\bGEMEINDE\\b|\\bGEMEINDEN\\b|\\bGEMEINDEEBENE\\b|\\bBEZIRK\\b|\\bBEZIRKSEBENE\\b|\\bBEZIRKE\\b|\\bKANTON\\b|\\bKANTONSEBENE\\b|\\bREGION\\b', sentence,flags=re.IGNORECASE)\n","            if match_span is not None:\n","                match_span=match_span.span()\n","                out_entities.append((match_span[0], match_span[1], \"GRAN\"))\n","\n","            #2) DATASET\n","            print(main)\n","            match_span = re.search(main,sentence,flags=re.IGNORECASE) \n","            print(match_span)\n","            if match_span is not None:\n","                match_span=match_span.span()\n","                out_entities.append((match_span[0], match_span[1], \"DATA\"))\n","\n","\n","            out_sentences.append((sentence, {\"entities\": out_entities}))\n","\n","    # for colab: list files that cannot be opened by google colab\n","    except:\n","        errors.append(i)\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQGgYfIlW6cH","scrolled":true},"source":["# save the generated sentences \n","print(out_sentences[:10])\n","with open(\"input/tagged_sentences.json\",\"w\",encoding='utf-8') as outfile:\n","    json.dump(out_sentences, outfile, ensure_ascii=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LThy3_cuW6cK"},"source":["# look at some sentences\n","print(out_sentences[1])\n","print(out_sentences[1][0][10])\n","print(out_sentences[1][0][12-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saOxPe9BW6cM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4FU2Z2PyM0D"},"source":[""],"execution_count":null,"outputs":[]}]}