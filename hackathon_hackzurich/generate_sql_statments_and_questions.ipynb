{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to SQL - Questions generation and testing for the Statbot-Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "INPUT_FOLDER = 'input_data'\n",
    "INDICATORS_VIEWS_FILE = 'INDICATORS_VIEWS.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NB_SAMPLES = 10 # number of samples per type (default: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_API_SECRET=%env NER_API_SECRET\n",
    "API_KEY=%env API_KEY\n",
    "DB_USER=%env DB_USER\n",
    "DB_PW=%env DB_PW\n",
    "DB_HOST=%env DB_HOST\n",
    "DB_PORT=%env DB_PORT\n",
    "DB_SCHEMA=\"public\"\n",
    "DB='hack_zurich'\n",
    "DRIVERNAME = \"postgresql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection with sqalchemy database\n",
    "engine = sqlalchemy.create_engine(\n",
    "    sqlalchemy.engine.url.URL(\n",
    "        drivername=DRIVERNAME,\n",
    "        username=DB_USER,\n",
    "        password=DB_PW,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB,\n",
    "    ),\n",
    "    echo_pool=True,\n",
    ")\n",
    "print(\"connecting with engine \" + str(engine))\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of natural language questions & SQL Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a dataset with indicator metadata and short descriptions which can be used to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.read_sql_query('SELECT * FROM indicators', connection)\n",
    "indicators = indicators.rename(columns={\"name\": \"indicator_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv(os.path.join(INPUT_FOLDER, INDICATORS_VIEWS_FILE))\n",
    "indicators = indicators.merge(descriptions, on='indicator_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single dataset with all information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a combined dataset containing all the information (values, indicator labels, spatial unit labels)  to make the generation of the question / sql pairs easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_values = pd.read_sql_query('SELECT * FROM indicator_values2', connection)\n",
    "gp_data = indicators_values.merge(indicators, on='indicator_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_units = pd.read_sql_query('SELECT * FROM spatialunit', connection)\n",
    "spatial_units = spatial_units.rename(columns={'name': 'municipality_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data = gp_data.merge(spatial_units, on='spatialunit_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with random values, years and municipalities per indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To fill in the questions and queries, random values, years and municipalities are drawn for each indicator. These are then integrated into the templates dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_units = spatial_units[['spatialunit_id', 'municipality_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data = gp_data[gp_data['type_id']==1]\n",
    "gp_data = gp_data[gp_data['question_type']<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one random sample of each group\n",
    "grouping_columns = ['indicator_id', 'indicator_name', 'short_description', 'question_type']\n",
    "grouped_samples = gp_data.groupby(by=grouping_columns)\n",
    "\n",
    "sample_columns = grouping_columns + ['value', 'year', 'spatialunit_id', 'view_name', 'view_column_value']\n",
    "samples = grouped_samples.sample(n=NB_SAMPLES).reset_index()[sample_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.merge(spatial_units, on='spatialunit_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vectors in the templates\n",
    "def create_question_query(sample: pd.Series) -> Tuple[List[str], List[str]]:\n",
    "    '''\n",
    "    From a sample values, create 7 questions and 7 associated queries\n",
    "    Args:\n",
    "        - sample: row of random observations values\n",
    "    Return:\n",
    "        - questions: list of question generated with the values from sample\n",
    "        - queries: : list of queries generated with the values from sample\n",
    "    '''\n",
    "\n",
    "    indicator, view, view_column, random_value, indicator_id, indicator_year, municipality = sample[\n",
    "        ['short_description', 'view_name', 'view_column_value', 'value', 'indicator_id', 'year', 'municipality_name']\n",
    "    ].values\n",
    "    \n",
    "    questions = [\n",
    "        f\"How high is the {indicator} in {municipality} in the year {indicator_year}?\", # 0\n",
    "        f\"Which municipality has the highest {indicator}?\", # 1\n",
    "        f\"Which municipality has the minimum {indicator}?\", # 2\n",
    "        f\"What are the highest, lowest and average {indicator}?\", # 3\n",
    "        f\"How many municipalities have a {indicator} higher than {random_value} per year?\", # 4\n",
    "        f\"How high is the total {indicator} in the Canton Zurich in the year {indicator_year}?\", # 5\n",
    "        f\"Which region had the lowest {indicator} in the year {indicator_year}?\" # 6\n",
    "    ]\n",
    "    \n",
    "    queries = [\n",
    "        # 0\n",
    "        f\"SELECT T1.{view_column} \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.name LIKE '{municipality}' AND T1.year = {indicator_year}\", \n",
    "        \n",
    "        # 1\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM spatialunit AS T2 \\\n",
    "        JOIN {view} AS T1 ON T2.spatialunit_id = T1.spatialunit_id \\\n",
    "        ORDER BY T1.{view_column} DESC LIMIT 1\",\n",
    "        \n",
    "        # 2\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM spatialunit AS T2 \\\n",
    "        JOIN {view} AS T1 ON T2.spatialunit_id = T1.spatialunit_id \\\n",
    "        ORDER BY T1.{view_column} ASC LIMIT 1\",\n",
    "        \n",
    "        # 3\n",
    "        f\"SELECT MAX(T1.{view_column}::numeric), \\\n",
    "        MIN(T1.{view_column}::numeric), AVG(T1.{view_column}::numeric) \\\n",
    "        FROM {view} AS T1\",\n",
    "        \n",
    "        # 4\n",
    "        f\"SELECT T1.year, COUNT(*) \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T1.{view_column}::numeric > {random_value} AND T2.type_id = 1 \\\n",
    "        GROUP BY T1.year\",\n",
    "        \n",
    "        # 5\n",
    "        f\"SELECT T1.{view_column} FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.type_id = 8 AND T1.year = {indicator_year}\",\n",
    "        \n",
    "        # 6\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.type_id = 4 AND T1.year = {indicator_year} \\\n",
    "        ORDER BY T1.{view_column} \\\n",
    "        LIMIT 1\"\n",
    "    ]\n",
    "    return questions, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 7 questions and queries for each sample\n",
    "all_questions, all_queries = [], []\n",
    "for _, sample in samples.iterrows():   \n",
    "    questions, queries = create_question_query(sample)\n",
    "    all_questions.extend(questions)\n",
    "    all_queries.extend(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save questions and queries\n",
    "df = pd.DataFrame({'questions': all_questions, 'queries': all_queries})\n",
    "df.to_csv(\"questions_queries_python.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the SQL queries on Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv containing the queries\n",
    "df = pd.read_csv(\"questions_queries_python.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(df: pd.DataFrame, query_number: int) -> None:\n",
    "    '''\n",
    "    Run a query on the database and prints the associted question and answer\n",
    "    Args:\n",
    "        - df: dataframe with all random samples\n",
    "        - query_number: index of the row of (question, query) to select\n",
    "    '''\n",
    "    question = df['questions'].iloc[query_number]\n",
    "    query = df['queries'].iloc[query_number]\n",
    "    answer = connection.execute(query)\n",
    "    \n",
    "    print(f\"Question {query_number}: {question}\")\n",
    "    print(f\"Answer {query_number}: {[r for r in answer]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(df, query_number=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    run_query(df, i)\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"questions_queries_python.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML-Model taken from https://github.com/Vamsi995/Paraphrase-Generator\n",
    "# https://huggingface.co/Vamsi/T5_Paraphrase_Paws\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrases(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Script for creating paraphrases and thus expanding the q&a pairs generated through the first script\n",
    "    At the time of writing it generates from 90 initial q&a pairs, around \n",
    "\n",
    "    ML-Model taken from https://github.com/Vamsi995/Paraphrase-Generator\n",
    "    https://huggingface.co/Vamsi/T5_Paraphrase_Paws\n",
    "\n",
    "    Version 0.1.1 - 15.09.2021\n",
    "    Christian Ruiz - Statistisches Amt Kanton Zürich\n",
    "    CC0\n",
    "    \n",
    "    History \n",
    "    Version 0.1.2 -15.09.2021 - Umlaut-corrections for the SQL-queries\n",
    "    Version 0.1.1 -15.09.2021 - First version public\n",
    "    '''\n",
    "    output_df = df\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sentence=df['questions'].iloc[i]\n",
    "        print(i, \" \", sentence)\n",
    "        text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "        encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=256,\n",
    "            do_sample=True,\n",
    "            top_k=120,\n",
    "            top_p=0.95,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=8\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            output_df = output_df.append({'questions': line, 'queries': df['queries'].iloc[i]}, ignore_index=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = generate_paraphrases(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments\n",
    "output_df = output_df[['questions','queries']]\n",
    "output_df['questions'] = output_df['questions'].str.lower()\n",
    "output_df = output_df.drop_duplicates()\n",
    "\n",
    "# Replace the Umlaut in the SQL-queries for ValueNet\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ü\", \"ue\")\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ä\", \"ae\")\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ö\", \"oe\")\n",
    "\n",
    "# Save paraphrases\n",
    "output_df.to_csv(\"questions_queries_paraphrases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to spider format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv containing the queries\n",
    "df = pd.read_csv(\"questions_queries_paraphrases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_spider_format(df: pd.DataFrame) -> list:\n",
    "    handmade_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        row_dict = {\n",
    "            'db_id': 'hack_zurich',\n",
    "            'query': row['queries'],\n",
    "            'question': row['questions']\n",
    "        }\n",
    "        handmade_data.append(row_dict)\n",
    "    return handmade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handmade_data = df_to_spider_format(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save question_queries in the format required by valuenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "training_samples = int(len(handmade_data)*TRAIN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = handmade_data[:training_samples]\n",
    "dev_data = handmade_data[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('handmade_data_train.json', 'w') as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "\n",
    "with open('handmade_data_dev.json', 'w') as outfile:\n",
    "    json.dump(dev_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to train Valuenet with this data, you can follow the steps explained in [preprocess_custom_data-01.ipynb](https://github.com/hack-with-admin-ch/aws-sagemaker-notebook-valuenet/blob/training_options/preprocess_custom_data-01.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_valuenet",
   "language": "python",
   "name": "conda_valuenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
