{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A03_train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Op9B9Rs0b_TP"},"source":["# 03 Train"]},{"cell_type":"markdown","metadata":{"id":"NIxuxCHCCKqE"},"source":["#### Connect to GD"]},{"cell_type":"code","metadata":{"id":"DEW4yvH5b_Fo"},"source":["# set up connection to your google drive\n","# please click on the link generated and enter the authorisation code\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRFDCydJb-61"},"source":["cd /content/drive/MyDrive/Colab/statbot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MS-ED5-oH1ue"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7DxBQscb-sn"},"source":["# Install / update spacy\n","# update spacy\n","! pip install -U spacy\n","! python -m spacy info"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua20d8Hkb-jV"},"source":["# load German model\n","! python -m spacy download de_core_news_lg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dxa9X5YCCKqa"},"source":["## DISCLAIMER: \n","A large part of this code for training new entities, based on a pre-trained model, is taken from Isaac Aderogba:  https://deepnote.com/publish/2cc2d19c-c3ac-4321-8853-0bcf2ef565b3 Statistics Canton Zurich humbly tried to adapt his code for our purposes of training statistically relevant entities.\n","\n","## Script A03\n","Training is being done based on a pre-trained German bert-base-multilingual-cased model by using transfer learning. At the end of the script there is also an evaluation of the tags. This includes both the previously included NER tags and the new NER tags, as transfer learning can change the accuracy of previously learned tags.\n","\n","As large parts of this code have been taken from the link mentioned above, we recommend reading it. We consider it the best possible tutorial for this task.  \n","\n","## Thoughts for improvement\n","- **add link for POS** (Position of Speech)\n"]},{"cell_type":"code","metadata":{"id":"EpRM7dAgb8Lh"},"source":["from __future__ import print_function, unicode_literals\n","import spacy\n","import warnings\n","from spacy.util import minibatch, compounding\n","import de_core_news_lg\n","import pandas as pd\n","import numpy as np\n","from random import sample\n","import io, csv\n","import re\n","import random\n","import json\n","from spacy.training import Example\n","from spacy.tokens import Doc\n","from tqdm import tqdm\n","nlp = spacy.load('de_core_news_lg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPoTwYW1b8Ly"},"source":["# helper function for incrementing the revision counters\n","def increment_revision_counters(entity_counter, entities):\n","    for entity in entities:\n","        label = entity[2]\n","        if label in entity_counter:\n","            entity_counter[label] += 1\n","        else:\n","            entity_counter[label] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DHliIPnb8L0"},"source":["#out_sentences=open(\"input/tagged_sentences.csv\", \"r\").readlines()\n","with open('input/tagged_sentences.json') as json_file:\n","    out_sentences = json.load(json_file)\n","print(\"LENGTH OF DATASET: \",len(out_sentences))\n","dataset_dict={}\n","\n","for sent in out_sentences:\n","    entities = sent[1][\"entities\"]\n","    increment_revision_counters(dataset_dict, entities)\n","#    print(entity[1])\n","    #helper_dict.append(entity[1]['entities'][0][2])\n","#out_sentences[:5]\n","print(dataset_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uN4VTGBub8L5"},"source":["npr_df = pd.read_csv(\"external/deu_news_2015_3M-sentences.txt\", delimiter = \"\\t\")\n","npr_df=npr_df.sample(frac=1)\n","npr_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiHRyioDb8L-"},"source":["# *** <- lÃ¶schen?\n","# create an nlp object as we'll use this to seperate the sentences and identify existing entities\n","#loaded already above\n","#nlp = spacy.load('de_core_news_lg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSGrNK5hb8MA"},"source":["revision_texts = []\n","\n","#STAT: Important: THIS IS A HYPER-PARAMETER: Changing it will affect the accuracy of the result\n","hyper_para_how_many=100000\n","\n","# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n","for doc in tqdm(nlp.pipe(npr_df.iloc[:hyper_para_how_many,1], batch_size=30, disable=[\"tagger\", \"ner\"])):\n","    for sentence in doc.sents:\n","\n","        if  40 < len(sentence.text) < 80:\n","            # some of the sentences had excessive whitespace in between words, so we're trimming that\n","            revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAbowBWMb8MD"},"source":["revisions = []\n","\n","# Use the existing spaCy model to predict the entities, then append them to revision\n","for doc in nlp.pipe(revision_texts, batch_size=50, disable=[\"tagger\", \"parser\"]):\n","    \n","    # don't append sentences that have no entities\n","    if len(doc.ents) > 0:\n","        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"74VtWx6Kb8MF"},"source":["# print an example of the revision sentence\n","print(revisions[0][0])\n","\n","# print an example of the revision data\n","print(revisions[0][1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1nkMNnGb8MI"},"source":["# create arrays to store the revision data\n","TRAIN_REVISION_DATA = []\n","TEST_REVISION_DATA = []\n","\n","# create dictionaries to keep count of the different entities\n","TRAIN_ENTITY_COUNTER = {}\n","TEST_ENTITY_COUNTER = {}\n","\n","# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n","REVISION_SENTENCE_SOFT_LIMIT = 100\n","\n","\n","\n","random.shuffle(revisions)\n","for revision in revisions:\n","    # get the entities from the revision sentence\n","    entities = revision[1][\"entities\"]\n","\n","    # simple hack to make sure spaCy entities don't get too one-sided\n","    should_append_to_train_counter = 0\n","    for _, _, label in entities:\n","        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n","            should_append_to_train_counter -= 1\n","        else:\n","            should_append_to_train_counter += 1\n","\n","    # simple switch for deciding whether to append to train data or test data\n","    if should_append_to_train_counter >= 0:\n","        TRAIN_REVISION_DATA.append(revision)\n","        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n","    else:\n","        TEST_REVISION_DATA.append(revision)\n","        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tduOpWdFb8MK"},"source":["TRAIN_ENTITY_COUNTER"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvUqZc5qb8MM"},"source":["TEST_ENTITY_COUNTER"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Zu_Dpqeb8MR"},"source":["TRAIN_REVISION_DATA"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUq222v2b8MT"},"source":["random.shuffle(out_sentences)\n","TRAIN_STAT_DATA=out_sentences[:len(out_sentences)//2]\n","TEST_STAT_DATA=out_sentences[len(out_sentences)//2:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTD4yXbHb8MU"},"source":["print(len(out_sentences))\n","print(len(TRAIN_STAT_DATA))\n","print(len(TEST_STAT_DATA))\n","print(\"REVISION\", len(TRAIN_REVISION_DATA))\n","TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_STAT_DATA\n","print(\"COMBINED\", len(TRAIN_DATA))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EA9VhQFb8MV"},"source":["#STAT: below is the heart piece of this script, and the code was heavily changed compared to the original\n","#script taken out of the code on deepnote.com. The reason is thaat this code has been adapted to spacy 3 -\n","#while the old code was running on spacy 2.X\n","#central command is nlp-update\n","\n","ner = nlp.get_pipe(\"ner\")\n","ner.add_label(\"GRAN\")\n","ner.add_label(\"DATA\")\n","\n","\n","\n","# get the names of the components we want to disable during training\n","pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n","other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n","\n","# start the training loop, only training NER\n","epochs = 30\n","#optimizer = nlp.resume_training()\n","#optimizer = nlp.initialize()\n","with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n","    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n","    sizes = compounding(1.0, 4.0, 1.001)\n","    \n","    # batch up the examples using spaCy's minibatc\n","    for epoch in range(epochs):\n","        random.shuffle(TRAIN_DATA)\n","        #text = []\n","        #annots=[]\n","        examples=[]\n","\n","\n","        for text,annots in TRAIN_DATA:\n","            #text.append(t)\n","            #annots.append(a)\n","            doc = nlp.make_doc(text)    \n","            example = Example.from_dict(doc, annots)\n","            examples.append(example)\n","        \n","        losses = {}\n","        \n","        nlp.update(examples, drop=0.35, losses=losses)#,sgd=optimizer)\n","\n","        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_RIUKQTb8MX","scrolled":false},"source":["statbot_colors = {\"GRAN\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n","                  \"DATA\": \"linear-gradient(90deg, #ffff00, #ff8c00)\"}\n","statbot_options = {\"ents\": [\"PER\",\"LOC\",\"ORG\",\"MISC\",\"GRAN\",\"DATA\"], \"colors\": statbot_colors}\n","spacy.displacy.render(nlp(\"Ich heisse Christian und war heute in ZÃ¼rich bei IBM im Internet.\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Wie viele KÃ¼he hat die Gemeinde BÃ¼lach?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Wie hoch ist Eigenkapital auf Bezirksebene?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Ich brauche die Daten pro Bezirk\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Ich brauche die Daten fÃ¼r den gesamten Kanton.\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Wie viel Bauinv. EFH 5 Jahre  hat  in Regensdorf  ?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Was ist der Anteil an MIV-Anteil (Modal Split)   auf Bezirksebene ?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Was ist der Anteil an Geb.Vol. Dienstleistungen: Zunahme   in Flaach ?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Welches ist das SchÃ¼l. Sekundarstufe II   fÃ¼r den gesamten Kanton ?\"), style=\"ent\",options=statbot_options)\n","spacy.displacy.render(nlp(\"Welche Gemeinde hat die grÃ¶sste BevÃ¶lkerung?\"), style=\"ent\",options=statbot_options)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owtuVlcab8MY"},"source":["#now check the accuracy of our NERs in this plus the next code chunks!\n","\n","\n","# dictionary to hold our evaluation data\n","stat_evaluation = {\n","    \"GRAN\": {\n","        \"correct\": 0,\n","        \"total\": 0,\n","    },\n","    \"DATA\": {\n","        \"correct\": 0,\n","        \"total\": 0,\n","    }\n","}\n","\n","word_evaluation = {\n","    \"GRAN\": {\n","        \"correct\": 0,\n","        \"total\": 0\n","    },\n","    \"DATA\": {\n","        \"correct\": 0,\n","        \"total\": 0,\n","    }\n","\n","}\n","\n","\n","for stat in TEST_STAT_DATA:\n","    # extract the sentence and correct stat entities according to our test data\n","    sentence = stat[0]\n","    entities = stat[1][\"entities\"]\n","\n","    # for each entity, use our updated model to make a prediction on the sentence\n","    for entity in entities:\n","        doc = nlp(sentence)\n","        correct_text = sentence[entity[0]:entity[1]]\n","        n_worded_stat =  len(correct_text.split())\n","        print(n_worded_stat)\n","\n","        # if we find that there's a match for predicted entity and predicted text, increment correct counters\n","        for ent in doc.ents:\n","            print(\"ENT_LABEL\",ent.label_)\n","            print(\"ENTITY2\",entity[2])\n","            print(\"ENT_TEXT\",ent.text)\n","            print(\"CORRECT:TEXT\",correct_text)\n","            if ent.label_ == entity[2] and ent.text == correct_text:\n","                \n","                stat_evaluation[entity[2]][\"correct\"] += 1\n","                if n_worded_stat > 0:\n","                    word_evaluation[entity[2]][\"correct\"] += 1\n","\n","                # this break is important, ensures that we're not double counting on a correct match\n","                break\n","\n","        #  increment total counters after each entity loop\n","        stat_evaluation[entity[2]][\"total\"] += 1\n","        if n_worded_stat > 0:\n","            word_evaluation[entity[2]][\"total\"] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VzZ4HP6Eb8Mb"},"source":["for key in word_evaluation:\n","    correct = word_evaluation[key][\"correct\"]\n","    total = word_evaluation[key][\"total\"]\n","\n","    print(f\"{key}: {correct / total * 100:.2f}%\")\n","\n","stat_total_sum = 0\n","stat_correct_sum = 0\n","\n","print(\"---\")\n","for key in stat_evaluation:\n","    correct = stat_evaluation[key][\"correct\"]\n","    total = stat_evaluation[key][\"total\"]\n","    \n","    stat_total_sum += total\n","    stat_correct_sum += correct\n","\n","    print(f\"{key}: {correct / total * 100:.2f}%\")\n","\n","print(f\"\\nTotal: {stat_correct_sum/stat_total_sum * 100:.2f}%\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fhfB65gb8Md"},"source":["#now test the accuracy of all the old NERs - was there amnesia on them?\n","\n","# dictionary which will be populated with the entities and result information\n","entity_evaluation = {}\n","\n","# helper function to udpate the entity_evaluation dictionary\n","def update_results(entity, metric):\n","    if entity not in entity_evaluation:\n","        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n","    \n","    entity_evaluation[entity][metric] += 1\n","\n","# same as before, see if entities from test set match what spaCy currently predicts\n","for data in TEST_REVISION_DATA:\n","    sentence = data[0]\n","    entities = data[1][\"entities\"]\n","\n","    for entity in entities:\n","        doc = nlp(sentence)\n","        correct_text = sentence[entity[0]:entity[1]]\n","\n","        for ent in doc.ents:\n","            if ent.label_ == entity[2] and ent.text == correct_text:\n","                update_results(ent.label_, \"correct\")\n","                break\n","\n","        update_results(entity[2], \"total\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r34g33cob8Mf"},"source":["sum_total = 0\n","sum_correct = 0\n","\n","for entity in entity_evaluation:\n","    total = entity_evaluation[entity][\"total\"]\n","    correct = entity_evaluation[entity][\"correct\"]\n","\n","    sum_total += total\n","    sum_correct += correct\n","    \n","    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n","\n","print()\n","print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgCG-oqEb8Mj"},"source":["nlp.meta[\"name\"] = \"stat_entity_extractor_v1\"\n","nlp.to_disk(\"./models/v1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rX0e1rXqb8Mz"},"source":["TRAIN_STAT_DATA[5]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EY1eJev0b8M0","scrolled":true},"source":["doc = nlp(u'Welche Gemeinde hat die grÃ¶sste BevÃ¶lkerung und welche hatte im 2019 den hÃ¶chsten AuslÃ¤nderanteil?')\n","\n","# show universal pos tags\n","print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.pos_) for t in doc))\n","# output: Ich/PRON bin/AUX ein/DET Berliner/NOUN ./PUNCT\n","\n","# show German specific pos tags (STTS)\n","print(' '.join('{word}/{tag}'.format(word=t.orth_, tag=t.tag_) for t in doc))\n","# output: Ich/PPER bin/VAFIN ein/ART Berliner/NN ./$.\n","\n","# show dependency arcs\n","print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, label=t.dep_, head=t.head.orth_) for t in doc))\n","# output: (sb: subject, nk: noun kernel, pd: predicate)\n","\n","#named entities\n","print(\"Named Entity Recognition:\")\n","for ent in doc.ents:\n","    print(ent.text)\n","print(\"Noun chunks:\")\n","for chunk in doc.noun_chunks:\n","    print(chunk.text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKoFE6FNb8M1"},"source":[""],"execution_count":null,"outputs":[]}]}